{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Warehousing and Data Mining\n",
    "\n",
    "## Labs\n",
    "\n",
    "### Prepared by Gilroy Gordon\n",
    "\n",
    "#### Contact Information\n",
    "\n",
    "SCIT ext. 3643\n",
    "\n",
    "ggordonutech@gmail.com\n",
    "\n",
    "gilroy.gordon@utech.edu.jm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 - Neural Networks in Python\n",
    "\n",
    "\n",
    "Additional Reference Resources:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "Objectives\n",
    "----\n",
    "---\n",
    "     > Data Preprocessing\n",
    "          > Min Max Scaling\n",
    "     > Data Transformation\n",
    "     > Data Mining\n",
    "          > Neural Networks (Classification and Estimation)\n",
    "     > Model Evaluation and Prediction\n",
    "          > Train/Test Split - 70/30\n",
    "     > Presentation\n",
    "          > Plots\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6</td>\n",
       "      <td>247</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5</td>\n",
       "      <td>259</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5</td>\n",
       "      <td>224</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6</td>\n",
       "      <td>305</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                 0.38             0.53               2                   157   \n",
       "1                 0.80             0.86               5                   262   \n",
       "2                 0.11             0.88               7                   272   \n",
       "3                 0.72             0.87               5                   223   \n",
       "4                 0.37             0.52               2                   159   \n",
       "5                 0.41             0.50               2                   153   \n",
       "6                 0.10             0.77               6                   247   \n",
       "7                 0.92             0.85               5                   259   \n",
       "8                 0.89             1.00               5                   224   \n",
       "9                 0.42             0.53               2                   142   \n",
       "10                0.45             0.54               2                   135   \n",
       "11                0.11             0.81               6                   305   \n",
       "12                0.84             0.92               4                   234   \n",
       "13                0.41             0.55               2                   148   \n",
       "14                0.36             0.56               2                   137   \n",
       "\n",
       "    time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n",
       "0                    3              0     1                      0  sales   \n",
       "1                    6              0     1                      0  sales   \n",
       "2                    4              0     1                      0  sales   \n",
       "3                    5              0     1                      0  sales   \n",
       "4                    3              0     1                      0  sales   \n",
       "5                    3              0     1                      0  sales   \n",
       "6                    4              0     1                      0  sales   \n",
       "7                    5              0     1                      0  sales   \n",
       "8                    5              0     1                      0  sales   \n",
       "9                    3              0     1                      0  sales   \n",
       "10                   3              0     1                      0  sales   \n",
       "11                   4              0     1                      0  sales   \n",
       "12                   5              0     1                      0  sales   \n",
       "13                   3              0     1                      0  sales   \n",
       "14                   3              0     1                      0  sales   \n",
       "\n",
       "    salary  \n",
       "0      low  \n",
       "1   medium  \n",
       "2   medium  \n",
       "3      low  \n",
       "4      low  \n",
       "5      low  \n",
       "6      low  \n",
       "7      low  \n",
       "8      low  \n",
       "9      low  \n",
       "10     low  \n",
       "11     low  \n",
       "12     low  \n",
       "13     low  \n",
       "14     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data/hr_data.csv' # Path to data file\n",
    "data = pd.read_csv(data_path) \n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n",
       "       'promotion_last_5years', 'sales', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What columns are in the data set ? Do they have spaces that I should consider\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim: Can we determine a person's `Satisfaction Level` based on the other factors?\n",
    "\n",
    "satisfaction_level = a(last_evaluation) + b(number_project) + c(average_montly_hours) + d(time_spend_company)\n",
    "\n",
    "The coefficients a-d, what are they? What is the relationship between the variables? Does multicolinearity exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a function below ```create_label_encoder_dict``` to assist with this. The function accepts a dataframe object and uses the ```LabelEncoder``` class from ```sklearn.preprocessing``` to encode (dummy encoding) or transform non-numerical columns to numbers. Finally it returns a dictionary object of all the encoders created for each column.\n",
    "\n",
    "The LabelEncoder is a useful resource as it not only automatically transforms all values in a column but also keeps a track of what values were transformed from. i.e. It will change all ```Female``` to ```0``` and all ```Male``` to ```1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_encoder_dict(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    label_encoder_dict = {}\n",
    "    for column in df.columns:\n",
    "        # Only create encoder for categorical data types\n",
    "        if not np.issubdtype(df[column].dtype, np.number) and column != 'Age':\n",
    "            label_encoder_dict[column]= LabelEncoder().fit(df[column])\n",
    "    return label_encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Values for each Label\n",
      "================================\n",
      "================================\n",
      "Encoder(sales) = ['IT' 'RandD' 'accounting' 'hr' 'management' 'marketing' 'product_mng'\n",
      " 'sales' 'support' 'technical']\n",
      "             Encoded Values\n",
      "IT                        0\n",
      "RandD                     1\n",
      "accounting                2\n",
      "hr                        3\n",
      "management                4\n",
      "marketing                 5\n",
      "product_mng               6\n",
      "sales                     7\n",
      "support                   8\n",
      "technical                 9\n",
      "================================\n",
      "Encoder(salary) = ['high' 'low' 'medium']\n",
      "        Encoded Values\n",
      "high                 0\n",
      "low                  1\n",
      "medium               2\n"
     ]
    }
   ],
   "source": [
    "label_encoders = create_label_encoder_dict(data)\n",
    "print(\"Encoded Values for each Label\")\n",
    "print(\"=\"*32)\n",
    "for column in label_encoders:\n",
    "    print(\"=\"*32)\n",
    "    print('Encoder(%s) = %s' % (column, label_encoders[column].classes_ ))\n",
    "    print(pd.DataFrame([range(0,len(label_encoders[column].classes_))], columns=label_encoders[column].classes_, index=['Encoded Values']  ).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data set\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6</td>\n",
       "      <td>247</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5</td>\n",
       "      <td>259</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5</td>\n",
       "      <td>224</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6</td>\n",
       "      <td>305</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>4</td>\n",
       "      <td>234</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                 0.38             0.53               2                   157   \n",
       "1                 0.80             0.86               5                   262   \n",
       "2                 0.11             0.88               7                   272   \n",
       "3                 0.72             0.87               5                   223   \n",
       "4                 0.37             0.52               2                   159   \n",
       "5                 0.41             0.50               2                   153   \n",
       "6                 0.10             0.77               6                   247   \n",
       "7                 0.92             0.85               5                   259   \n",
       "8                 0.89             1.00               5                   224   \n",
       "9                 0.42             0.53               2                   142   \n",
       "10                0.45             0.54               2                   135   \n",
       "11                0.11             0.81               6                   305   \n",
       "12                0.84             0.92               4                   234   \n",
       "13                0.41             0.55               2                   148   \n",
       "14                0.36             0.56               2                   137   \n",
       "\n",
       "    time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n",
       "0                    3              0     1                      0      7   \n",
       "1                    6              0     1                      0      7   \n",
       "2                    4              0     1                      0      7   \n",
       "3                    5              0     1                      0      7   \n",
       "4                    3              0     1                      0      7   \n",
       "5                    3              0     1                      0      7   \n",
       "6                    4              0     1                      0      7   \n",
       "7                    5              0     1                      0      7   \n",
       "8                    5              0     1                      0      7   \n",
       "9                    3              0     1                      0      7   \n",
       "10                   3              0     1                      0      7   \n",
       "11                   4              0     1                      0      7   \n",
       "12                   5              0     1                      0      7   \n",
       "13                   3              0     1                      0      7   \n",
       "14                   3              0     1                      0      7   \n",
       "\n",
       "    salary  \n",
       "0        1  \n",
       "1        2  \n",
       "2        2  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply each encoder to the data set to obtain transformed values\n",
    "data2 = data.copy() # create copy of initial data set\n",
    "for column in data2.columns:\n",
    "    if column in label_encoders:\n",
    "        data2[column] = label_encoders[column].transform(data2[column])\n",
    "\n",
    "print(\"Transformed data set\")\n",
    "print(\"=\"*32)\n",
    "data2.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our data into dependent (Y) and independent(X) variables\n",
    "X_data = data2[['last_evaluation','number_project','average_montly_hours','time_spend_company']]\n",
    "Y_data = data2['satisfaction_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70/30 Train Test Split\n",
    "\n",
    "We will split the data using a 70/30 split. i.e. 70% of the data will be randomly chosen to train the model\n",
    "and 30% will be used to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of linear regression\n",
    "reg = MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPRegressor in module sklearn.neural_network.multilayer_perceptron:\n",
      "\n",
      "class MLPRegressor(BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      " |  Multi-layer Perceptron regressor.\n",
      " |  \n",
      " |  This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
      " |  descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
      " |        Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, optional, default 0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, optional, default 'auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when solver='sgd'.\n",
      " |  \n",
      " |  learning_rate_init : double, optional, default 0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, optional, default 0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, optional, default 200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, optional, default True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      " |      is set to 'adaptive', convergence is considered to be reached and\n",
      " |      training stops.\n",
      " |  \n",
      " |  verbose : bool, optional, default False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, optional, default False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  \n",
      " |  momentum : float, default 0.9\n",
      " |      Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : boolean, default True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for two consecutive\n",
      " |      epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, optional, default 0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, optional, default 0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, optional, default 1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  coefs_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int,\n",
      " |      The number of iterations the solver has ran.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : string\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPRegressor trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense and sparse numpy\n",
      " |  arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPRegressor\n",
      " |      BaseMultilayerPerceptron\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like, shape (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Fit the model to data matrix X and target y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.n_layers_ # Number of layers utilized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66908448, 0.65429664, 0.60327367, ..., 0.47367503, 0.72666922,\n",
       "       0.57838912])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "test_predicted = reg.predict(X_test)\n",
    "test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>predicted_satisfaction_level</th>\n",
       "      <th>satisfaction_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6335</th>\n",
       "      <td>0.81</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>0.669084</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>0.73</td>\n",
       "      <td>3</td>\n",
       "      <td>138</td>\n",
       "      <td>3</td>\n",
       "      <td>0.654297</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8864</th>\n",
       "      <td>0.56</td>\n",
       "      <td>3</td>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>0.603274</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>0.57</td>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>0.577754</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13361</th>\n",
       "      <td>0.61</td>\n",
       "      <td>4</td>\n",
       "      <td>118</td>\n",
       "      <td>5</td>\n",
       "      <td>0.541316</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       last_evaluation  number_project  average_montly_hours  \\\n",
       "6335              0.81               5                   143   \n",
       "7020              0.73               3                   138   \n",
       "8864              0.56               3                   142   \n",
       "7454              0.57               4                   141   \n",
       "13361             0.61               4                   118   \n",
       "\n",
       "       time_spend_company  predicted_satisfaction_level  satisfaction_level  \n",
       "6335                    2                      0.669084                0.80  \n",
       "7020                    3                      0.654297                0.62  \n",
       "8864                    3                      0.603274                0.81  \n",
       "7454                    3                      0.577754                0.49  \n",
       "13361                   5                      0.541316                0.54  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = X_test.copy()\n",
    "data3['predicted_satisfaction_level']=test_predicted\n",
    "data3['satisfaction_level']=y_test\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.06\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error don't worry guys we can do this\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance score: 0.07\n"
     ]
    }
   ],
   "source": [
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_min_max_scaler_dict(df):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler_dict = {}\n",
    "    for column in df.columns:\n",
    "        # Only create encoder for categorical data types\n",
    "        if np.issubdtype(df[column].dtype, np.number):\n",
    "            min_max_scaler_dict[column]= MinMaxScaler().fit(pd.DataFrame(df[column]))\n",
    "    return min_max_scaler_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Max Values for each Label\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Work_accident': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'average_montly_hours': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'last_evaluation': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'left': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'number_project': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'promotion_last_5years': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'satisfaction_level': MinMaxScaler(copy=True, feature_range=(0, 1)),\n",
       " 'time_spend_company': MinMaxScaler(copy=True, feature_range=(0, 1))}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scalers = create_min_max_scaler_dict(data)\n",
    "print(\"Min Max Values for each Label\")\n",
    "print(\"=\"*32)\n",
    "min_max_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving a scacler\n",
    "time_spend_company_scaler=min_max_scalers['time_spend_company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_spend_company_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_spend_company_scaler.data_max_ #Maximum value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_spend_company_scaler.data_min_ # Minimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_spend_company_scaler.data_range_ # Range = Max- Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>promotion_last_5years</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time_spend_company</td>\n",
       "      <td>10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>number_project</td>\n",
       "      <td>7</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>left</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>last_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>average_montly_hours</td>\n",
       "      <td>310</td>\n",
       "      <td>96.00</td>\n",
       "      <td>214.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>satisfaction_level</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Work_accident</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  column  max    min   range\n",
       "0  promotion_last_5years    1   0.00    1.00\n",
       "1     time_spend_company   10   2.00    8.00\n",
       "2         number_project    7   2.00    5.00\n",
       "3                   left    1   0.00    1.00\n",
       "4        last_evaluation    1   0.36    0.64\n",
       "5   average_montly_hours  310  96.00  214.00\n",
       "6     satisfaction_level    1   0.09    0.91\n",
       "7          Work_accident    1   0.00    1.00"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([\n",
    "    {\n",
    "        'column':col,\n",
    "        'min':min_max_scalers[col].data_min_[0], \n",
    "        'max':min_max_scalers[col].data_max_[0], \n",
    "        'range':min_max_scalers[col].data_range_[0] }  for col in min_max_scalers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data set\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285047</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.593458</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.294393</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266355</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.705607</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.879121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.395604</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182243</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.976636</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191589</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0             0.318681         0.265625             0.0              0.285047   \n",
       "1             0.780220         0.781250             0.6              0.775701   \n",
       "2             0.021978         0.812500             1.0              0.822430   \n",
       "3             0.692308         0.796875             0.6              0.593458   \n",
       "4             0.307692         0.250000             0.0              0.294393   \n",
       "5             0.351648         0.218750             0.0              0.266355   \n",
       "6             0.010989         0.640625             0.8              0.705607   \n",
       "7             0.912088         0.765625             0.6              0.761682   \n",
       "8             0.879121         1.000000             0.6              0.598131   \n",
       "9             0.362637         0.265625             0.0              0.214953   \n",
       "10            0.395604         0.281250             0.0              0.182243   \n",
       "11            0.021978         0.703125             0.8              0.976636   \n",
       "12            0.824176         0.875000             0.4              0.644860   \n",
       "13            0.351648         0.296875             0.0              0.242991   \n",
       "14            0.296703         0.312500             0.0              0.191589   \n",
       "\n",
       "    time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n",
       "0                0.125              0     1                      0      7   \n",
       "1                0.500              0     1                      0      7   \n",
       "2                0.250              0     1                      0      7   \n",
       "3                0.375              0     1                      0      7   \n",
       "4                0.125              0     1                      0      7   \n",
       "5                0.125              0     1                      0      7   \n",
       "6                0.250              0     1                      0      7   \n",
       "7                0.375              0     1                      0      7   \n",
       "8                0.375              0     1                      0      7   \n",
       "9                0.125              0     1                      0      7   \n",
       "10               0.125              0     1                      0      7   \n",
       "11               0.250              0     1                      0      7   \n",
       "12               0.375              0     1                      0      7   \n",
       "13               0.125              0     1                      0      7   \n",
       "14               0.125              0     1                      0      7   \n",
       "\n",
       "    salary  \n",
       "0        1  \n",
       "1        2  \n",
       "2        2  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply each scaler to the data set to obtain transformed values\n",
    "data3 = data2.copy() # create copy of initial data set\n",
    "for column in data3.columns:\n",
    "    if column in min_max_scalers:\n",
    "        data3[column] = min_max_scalers[column].transform(pd.DataFrame(data3[column]))\n",
    "\n",
    "print(\"Transformed data set\")\n",
    "print(\"=\"*32)\n",
    "data3.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.574542</td>\n",
       "      <td>0.556409</td>\n",
       "      <td>0.360611</td>\n",
       "      <td>0.490889</td>\n",
       "      <td>0.187279</td>\n",
       "      <td>0.144610</td>\n",
       "      <td>0.238083</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>5.870525</td>\n",
       "      <td>1.347290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.273220</td>\n",
       "      <td>0.267452</td>\n",
       "      <td>0.246518</td>\n",
       "      <td>0.233379</td>\n",
       "      <td>0.182517</td>\n",
       "      <td>0.351719</td>\n",
       "      <td>0.425924</td>\n",
       "      <td>0.144281</td>\n",
       "      <td>2.868786</td>\n",
       "      <td>0.625819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.696262</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "count        14999.000000     14999.000000    14999.000000   \n",
       "mean             0.574542         0.556409        0.360611   \n",
       "std              0.273220         0.267452        0.246518   \n",
       "min              0.000000         0.000000        0.000000   \n",
       "25%              0.384615         0.312500        0.200000   \n",
       "50%              0.604396         0.562500        0.400000   \n",
       "75%              0.802198         0.796875        0.600000   \n",
       "max              1.000000         1.000000        1.000000   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident          left  \\\n",
       "count          14999.000000        14999.000000   14999.000000  14999.000000   \n",
       "mean               0.490889            0.187279       0.144610      0.238083   \n",
       "std                0.233379            0.182517       0.351719      0.425924   \n",
       "min                0.000000            0.000000       0.000000      0.000000   \n",
       "25%                0.280374            0.125000       0.000000      0.000000   \n",
       "50%                0.485981            0.125000       0.000000      0.000000   \n",
       "75%                0.696262            0.250000       0.000000      0.000000   \n",
       "max                1.000000            1.000000       1.000000      1.000000   \n",
       "\n",
       "       promotion_last_5years         sales        salary  \n",
       "count           14999.000000  14999.000000  14999.000000  \n",
       "mean                0.021268      5.870525      1.347290  \n",
       "std                 0.144281      2.868786      0.625819  \n",
       "min                 0.000000      0.000000      0.000000  \n",
       "25%                 0.000000      4.000000      1.000000  \n",
       "50%                 0.000000      7.000000      1.000000  \n",
       "75%                 0.000000      8.000000      2.000000  \n",
       "max                 1.000000      9.000000      2.000000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our data into dependent (Y) and independent(X) variables\n",
    "X2_data = data3[['last_evaluation','number_project','average_montly_hours','time_spend_company']]\n",
    "Y2_data = data3['satisfaction_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70/30 Train Test Split\n",
    "\n",
    "We will split the data using a 70/30 split. i.e. 70% of the data will be randomly chosen to train the model\n",
    "and 30% will be used to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_data, Y2_data, test_size=0.30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of linear regression\n",
    "reg2 = MLPRegressor()\n",
    "reg2.fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57568627, 0.57893697, 0.5534623 , ..., 0.55752807, 0.71729991,\n",
       "       0.58278402])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "test2_predicted = reg2.predict(X2_test)\n",
    "test2_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.05\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error don't worry guys we can do this\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y2_test, test2_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooray, we improved by approximately 0.01\n",
    "\n",
    "## Let's Visualize using a Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f1a7da71518>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a7dcaa6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.residplot(y2_test, test2_predicted,  color=\"g\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you try changing the amount of hidden layers??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
